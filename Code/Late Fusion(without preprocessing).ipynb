{"cells":[{"cell_type":"code","execution_count":null,"id":"Pxh6IqzmwWBf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5209,"status":"ok","timestamp":1726799191189,"user":{"displayName":"Tien Nguyen","userId":"09045779890012802355"},"user_tz":420},"id":"Pxh6IqzmwWBf","outputId":"5dfecf44-24fc-4366-f912-379face2ec17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.10/dist-packages (0.51.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.26.4)\n","Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.4.1)\n","Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.8.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.11.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.12.1)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"]}],"source":["!pip install keras-self-attention\n","!pip install scikeras"]},{"cell_type":"code","execution_count":null,"id":"e92910b8","metadata":{"id":"e92910b8"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","import random as python_random\n","# NLP Libraries\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","\n","# Sklearn\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n","\n","# TensorFlow and Keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import (Input, Dense, LSTM, Bidirectional, Embedding,\n","                                     Dropout, GlobalAveragePooling2D, GlobalMaxPool1D)\n","from tensorflow.keras.layers import Layer, Concatenate\n","\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.utils import to_categorical\n","\n","# Image Processing\n","from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Scikit-learn Keras Wrapper\n","from scikeras.wrappers import KerasClassifier\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","os.environ['PYTHONHASHSEED'] = str(42)\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n"]},{"cell_type":"code","execution_count":null,"id":"nBZaNlqnUTfv","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3649,"status":"ok","timestamp":1726799198837,"user":{"displayName":"Tien Nguyen","userId":"09045779890012802355"},"user_tz":420},"id":"nBZaNlqnUTfv","outputId":"07887495-4d99-4583-c75c-bba9e6ac91d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: path.py in /usr/local/lib/python3.10/dist-packages (12.5.0)\n","Requirement already satisfied: path in /usr/local/lib/python3.10/dist-packages (from path.py) (17.0.0)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!pip install -U path.py\n","import os\n","from path import Path\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"BPGDk3UaV3gZ","metadata":{"id":"BPGDk3UaV3gZ"},"outputs":[],"source":["def reset_random_seeds():\n","   np.random.seed(42)\n","   python_random.seed(42)\n","   tf.random.set_seed(42)\n","reset_random_seeds()"]},{"cell_type":"code","source":[],"metadata":{"id":"NRoIkYHZwwFm"},"id":"NRoIkYHZwwFm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Text data"],"metadata":{"id":"BX-t8ZOTwyNi"},"id":"BX-t8ZOTwyNi"},{"cell_type":"code","execution_count":null,"id":"e5eba253","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1726800113842,"user":{"displayName":"Tien Nguyen","userId":"09045779890012802355"},"user_tz":420},"id":"e5eba253","outputId":"56d65f85-d0a2-47d6-9a5f-43f56c46c9a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["The sentence 'i do. i was, uh... in the office, and i got a call ... uh, mayb ... close to nine o' clock, i don't remember, somewher between eight and nine, from rusty. he said, uh ... someon wa just on the side of the house, and he wa sort of not complet coherent, and said ... someon was, like, sleep on the side of the house, and he call the police, and he was... oh and then he had to go realli quickli to talk to the police, and then he end up call me back, and told me the whole stori of what had happened, which wa that, um... he had put ian in the car as he doe everi day around that time, and ian' car seat is on the right passeng side in the back, and as he walk around he smell gas, um... as he walk -- 'caus he sort of exit the garag a littl bit to get to hi side of the car, he smell gas, and, uh... our ga meter and pipe is on the side of the left side of the hous if you'r look at the front. so he walk over to that spot, it' near our air conditioners, and he said that there wa a man lie face-down on the ground.' has the highest number of words (221).\n"]}],"source":["path_lie = Path(\"/content/drive/MyDrive/Tien-297/Real-life_Deception_Detection_2016/Transcription/Deceptive\")\n","files_lie = [file for file in os.listdir(path_lie) if not file.startswith('.')]\n","text = []\n","for file in files_lie:\n","    with open(path_lie+\"/\"+file, 'r') as f:\n","        text.append(f.read().replace('\\r\\n', ''))\n","df = pd.DataFrame(text, columns = ['Speech'])\n","df['label'] = 'Lie'\n","\n","path_truth = Path(\"/content/drive/MyDrive/Tien-297/Real-life_Deception_Detection_2016/Transcription/Truthful\")\n","files_truth = [file for file in os.listdir(path_truth) if not file.startswith('.')]\n","\n","for file in files_truth:\n","    with open(path_truth+\"/\"+file, 'r', encoding=\"utf8\") as f:\n","        text = [f.read().replace('\\r\\n', ''),'Truth']\n","        df.loc[len(df.index)] = text\n","\n","# Features and labels\n","x = df.drop('label', axis=1)\n","y = df['label'].map({'Truth':0, 'Lie':1})\n","var = x.copy()\n","var.reset_index(inplace=True)\n","stemmer = PorterStemmer()\n","corpus = []\n","# Instead of preprocessing, use the raw text directly\n","#corpus = var['Speech'].tolist()\n","for i in range(0, len(var)):\n","    #review = re.sub('[^a-zA-Z]', ' ', var['Speech'][i])\n","    # Use the raw text without removing non-alphabetic characters\n","    review = var['Speech'][i]\n","\n","    # Convert to lowercase\n","    review = review.lower()\n","\n","    # Split into individual words\n","    review = review.split()\n","\n","    # Apply stemming to each word\n","    review = [stemmer.stem(word) for word in review]\n","\n","    # Join the words back into a sentence\n","    review = ' '.join(review)\n","\n","    # Append the processed review to the corpus\n","    corpus.append(review)\n","# Find the sentence with the highest number of words\n","max_sentence = max(corpus, key=lambda x: len(x.split()))\n","print(f\"The sentence '{max_sentence}' has the highest number of words ({len(max_sentence.split())}).\")\n","\n","# Convert sentences to one-hot encoding\n","vocab_size = 5000\n","encoding = [one_hot(words, vocab_size) for words in corpus]\n","\n","# Pad the sequences to ensure uniform input length\n","emb_doc = pad_sequences(encoding, padding='pre', maxlen=221)\n","\n","# Convert to NumPy arrays for further processing\n","text_features = np.array(emb_doc)\n","text_labels = np.array(y)\n"]},{"cell_type":"code","execution_count":null,"id":"oDSdSjJOUBsP","metadata":{"id":"oDSdSjJOUBsP"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"i68Zr2pTCMng","metadata":{"id":"i68Zr2pTCMng"},"source":["# Loading Audio data"]},{"cell_type":"code","execution_count":null,"id":"s4gKxHXqCTOa","metadata":{"id":"s4gKxHXqCTOa"},"outputs":[],"source":["# Define the directories for your two classes\n","dir_deceptive = Path(\"/content/drive/MyDrive/Tien-297/Real-life_Deception_Detection_2016/Images/Deceptive_v1/\")\n","dir_truthful = Path(\"/content/drive/MyDrive/Tien-297/Real-life_Deception_Detection_2016/Images/Truthful_v1\")\n","\n","# Initialize lists to store the images and labels\n","images = []\n","labels = []  # 0 for 'Deceptive', 1 for 'Truthful'\n","\n","# Function to load images from a directory and assign labels\n","def load_images_from_directory(directory, label):\n","    for filename in os.listdir(directory):\n","        if filename.endswith('.png'):  # or other file formats as needed\n","            img_path = os.path.join(directory, filename)\n","            img = Image.open(img_path).convert('RGB')  # Convert to RGB\n","            img = img.resize((224, 224))\n","            images.append(np.array(img))\n","            labels.append(label)\n","\n","# Load 'Deceptive' images\n","load_images_from_directory(dir_deceptive, 0)\n","\n","# Load 'Truthful' images\n","load_images_from_directory(dir_truthful, 1)\n","\n","# Convert to numpy arrays and preprocess\n","audio_features = np.array(images)\n","audio_labels = np.array(labels)\n"]},{"cell_type":"markdown","source":["**LATE FUSION**"],"metadata":{"id":"lC-2KjrPwDvf"},"id":"lC-2KjrPwDvf"},{"cell_type":"code","execution_count":null,"id":"i8ZzzAZxUUBe","metadata":{"id":"i8ZzzAZxUUBe"},"outputs":[],"source":["class LinearW(Layer):\n","    def __init__(self, num_inputs, **kwargs):\n","        super(LinearW, self).__init__(**kwargs)  # Pass additional keyword arguments to the superclass\n","        self.num_inputs = num_inputs\n","\n","    def build(self, input_shape):\n","        # Initialize weights to combine the inputs\n","        self.W = self.add_weight(\n","            name=\"fusion_weights\",  # give a name to the weights\n","            shape=(1, 1, self.num_inputs),\n","            initializer='uniform',\n","            trainable=True\n","        )\n","\n","    def call(self, inputs):\n","        # Ensure that 'inputs' is a list of tensors\n","        inputs = [tf.expand_dims(i, -1) for i in inputs]\n","        inputs = Concatenate(axis=-1)(inputs)\n","        weights = tf.nn.softmax(self.W, axis=-1)  # Apply softmax to learn which inputs are more important\n","        return tf.reduce_sum(weights * inputs, axis=-1)  # Weighted sum of inputs\n","\n","\n","# Audio Model Modification\n","def create_audio_model(input_tensor):\n","    # Ensure ResNet50 is set up correctly for the input tensor\n","    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n","    for layer in base_model.layers[:-20]:\n","        layer.trainable = False\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(128, activation='relu')(x)  # Output modified for feature fusion\n","    return x\n","\n","def create_text_model(input_tensor):\n","    # The input_tensor is already defined with its shape when calling this function\n","    x = Embedding(vocab_size, 40)(input_tensor)  # No need for input_length\n","    x = Bidirectional(LSTM(100))(x)\n","    x = Dense(128, activation='relu')(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"id":"9K0SeEu_2nbi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111221,"status":"ok","timestamp":1726800235997,"user":{"displayName":"Tien Nguyen","userId":"09045779890012802355"},"user_tz":420},"id":"9K0SeEu_2nbi","outputId":"8b805d32-df98-4d24-ef63-3ae374ef4a72"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------- Fold 1------------\n","Weight:  [[[0.49265066 0.5073494 ]]]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.0020\n","Test Accuracy: 1.0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","Test F1 Score: 1.0\n","---------------------------\n","--------------- Fold 2------------\n","Weight:  [[[0.50001985 0.49998015]]]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9583 - loss: 0.0966\n","Test Accuracy: 0.9583333134651184\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","Test F1 Score: 0.9565217391304348\n","---------------------------\n","--------------- Fold 3------------\n","Weight:  [[[0.5136772  0.48632288]]]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 5.4715e-36\n","Test Accuracy: 1.0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","Test F1 Score: 1.0\n","---------------------------\n","--------------- Fold 4------------\n","Weight:  [[[0.51651144 0.48348856]]]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8333 - loss: 4.2438\n","Test Accuracy: 0.8333333134651184\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","Test F1 Score: 0.8823529411764706\n","---------------------------\n","--------------- Fold 5------------\n","Weight:  [[[0.4972663  0.50273365]]]\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 5.5059e-16\n","Test Accuracy: 1.0\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","Test F1 Score: 1.0\n","---------------------------\n","Average Test Accuracy: 0.9583333253860473\n","Average Test F1 Score: 0.967774936061381\n","Mean weights across folds: [[[0.5040251  0.49597493]]]\n"]}],"source":["n_splits = 5\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","accuracies = []\n","f1_scores = []\n","weights_across_folds = []\n","\n","fold_no = 1\n","for train_index, test_index in kf.split(audio_features):\n","    # Split data for this fold\n","    print('--------------- Fold {}------------'.format(fold_no))\n","    audio_train, audio_test = audio_features[train_index], audio_features[test_index]\n","    text_train, text_test = text_features[train_index], text_features[test_index]\n","    y_train, y_test = audio_labels[train_index], audio_labels[test_index]  # Assuming audio_labels holds the target values\n","\n","    # Define input layers for audio and text data inside the loop to reset weights\n","    input_audio = Input(shape=(224, 224, 3))  # Audio spectrogram dimensions\n","    input_text = Input(shape=(221,))  # Length of sequence for text\n","\n","    # Create outputs from both models\n","    audio_output = create_audio_model(input_audio)\n","    text_output = create_text_model(input_text)\n","\n","    # Fusion Layer with LinearW initialized to handle 2 inputs\n","    combined_output = LinearW(num_inputs=2, name='fusion_layer')([audio_output, text_output])\n","\n","    # Final dense layer for classification\n","    final_output = Dense(1, activation='sigmoid')(combined_output)\n","\n","    # Compile the fusion model inside the loop\n","    fusion_model = Model(inputs=[input_audio, input_text], outputs=final_output)\n","    fusion_model.compile(optimizer=Adam(learning_rate = 0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Train the model\n","    history = fusion_model.fit(\n","        [audio_train, text_train], y_train,\n","        validation_data=([audio_test, text_test], y_test),\n","        epochs=100,\n","        batch_size=20,\n","        callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)],\n","        verbose=0\n","    )\n","    # After training, fetch weights\n","    weights = fusion_model.get_layer('fusion_layer').get_weights()[0]\n","    weights_normalized = tf.nn.softmax(weights).numpy()\n","    print('Weight: ', weights_normalized)\n","    weights_across_folds.append(weights_normalized)\n","\n","    # Evaluate model on test data\n","    test_loss, test_accuracy = fusion_model.evaluate([audio_test, text_test], y_test)\n","    print(f\"Test Accuracy: {test_accuracy}\")\n","\n","    # Predictions and classification report\n","    y_pred = fusion_model.predict([audio_test, text_test])\n","    y_pred = (y_pred > 0.5).astype(int)\n","    accuracies.append(test_accuracy)\n","    f1 = f1_score(y_test, y_pred)\n","    f1_scores.append(f1)\n","    print(f\"Test F1 Score: {f1}\")\n","\n","    fold_no += 1\n","    print('---------------------------')\n","# Calculate and print average metrics\n","average_accuracy = np.mean(accuracies)\n","average_f1 = np.mean(f1_scores)\n","print(f'Average Test Accuracy: {average_accuracy}')\n","print(f'Average Test F1 Score: {average_f1}')\n","# Analyze how weights vary across folds\n","weights_across_folds = np.array(weights_across_folds)\n","mean_weights = np.mean(weights_across_folds, axis=0)\n","print(\"Mean weights across folds:\", mean_weights)"]},{"cell_type":"code","execution_count":null,"id":"J--y8FtIA0uX","metadata":{"id":"J--y8FtIA0uX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}